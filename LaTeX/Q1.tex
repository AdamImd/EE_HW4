\subsection{Posterior Samplers for Inverse Problems}
for each of the following tasks, I got results for three images from each of the two datastes, for each of the four inverse problems. I will only be including one image from each dataset for each task in the report, but the full results are included in the zip file submission. Please refer to the figures for the images corresponding to each method and dataset. I will report the PSNR, SSIM, and LPIPS for the images shown in the report, but the full results across all three images are also included in the zip file submission.

I used my lab's server to generate the images for this task, which has an NVIDIA A6000, with 1TB ram and 128 CPU cores. 
% Step 1 (35 points): Posterior Samplers for Inverse Problems
% Your first task is to implement and compare different posterior sampling methods for solving
% inverse problems using a pre-trained ADM model1 (Dhariwal & Nichol, 2021) from HW3.
% However, unlike unconditional generation in HW3, posterior sampling aims to reconstruct an
% image that is both consistent with a given measurement (e.g., noisy masked or low resolution
% image) and plausible under the learned data prior.
% To help you get started, we have provided the skeleton code for restoration (hw4 step1 main.py)
% along with the ADM model codes and some helper functions (utils.py) under the folder
% step1 utils. Make sure to check the lines with the #TODO flag inside the hw4 step1 main.py
% file in order to complete the code. Some pointers about the model and code:
% † We have provided the CelebA-HQ and ImageNet pretrained ADM checkpoints (click).
% Please download these .pt files and place them under "./step1 utils/models/".
% 1https://github.com/openai/guided-diffusion
% 1
% † This code builds upon the unconditional generation framework from HW3 and uses the
% same ADM backbone, but trained on a different dataset. In this step, you will extend
% that framework from pure image generation to posterior sampling, where the diffusion
% model is conditioned on measurement consistency. Therefore, most of the information
% from the ADM setup in HW3 remains applicable.
% Algorithm 1 Posterior Sampling Strategies
% Require: T, y, { ˜σi}T
% i=1, η
% 1: xT ∼ N (0, I)
% 2: for t = T, ..., 1 do
% 3: ˆx0|t ← 1√ ¯αt · (xt − ˆϵθ(xt, t)√1 − ¯αt) ▷ Tweedie denoised estimate
% 4: x′
% t−1 ∼ p(xt−1|xt, ˆx0|t) ▷ DDIM sampling
% 5: xt−1 ← Projection or gradient update via x′
% t−1 and ˆx0|t to get closer to {x|y = Ax}
% 6: end for
% 7: return x0
% Figure 2: Four degradations that will be covered in this HW. You can change the box
% indices as long as you keep it 128×128 and at least 10 pixels away from the edges.
% For this step, your goals are:

\subsubsection{A: Predict $\hat{x_0}$ and sample ddim}
% (a) [4 pts] Implement predict x0 hat() and sample ddim() functions, effectively repro-
% ducing Alg. 1 excluding line 5, to ensure you can perform unconditional sampling from
% both pretrained ADM models. Use η = 1.0 throughout this assignment, thereby im-
% plementing the DDPM variant. Run the sampling process for 1000 steps and present
% five different samples from each network in a 2 × 5 grid.
% 2

I implemened the DDIM from HW3, the results are shown in Figure~\ref{fig:uncond_samples}. The images generated look qualitatively similar to those from HW3, and the model for each dataset creates images from that distrobution.

\begin{figure*}[ht]
    \centering
    % CelebA-HQ samples (top row)
    \includegraphics[width=0.18\textwidth]{images/step1_results/CelebA_HQ/unconditional/sample_1.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/CelebA_HQ/unconditional/sample_2.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/CelebA_HQ/unconditional/sample_3.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/CelebA_HQ/unconditional/sample_4.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/CelebA_HQ/unconditional/sample_5.png}
    \\[2mm]
    % ImageNet samples (bottom row)
    \includegraphics[width=0.18\textwidth]{images/step1_results/ImageNet/unconditional/sample_1.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/ImageNet/unconditional/sample_2.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/ImageNet/unconditional/sample_3.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/ImageNet/unconditional/sample_4.png}\hspace{2mm}
    \includegraphics[width=0.18\textwidth]{images/step1_results/ImageNet/unconditional/sample_5.png}
    \caption{Unconditional samples from CelebA-HQ (top row) and ImageNet (bottom row) pretrained ADM models using DDIM sampling with 1000 steps.}
    \label{fig:uncond_samples}
\end{figure*}




\subsubsection{B: Implement and compare posterior samplers}
% (b) [8 pts] As discussed in the class, earlier attempts focused on direct projections onto
% the constraint set. For example, Iterative Latent Variable Refinement (ILVR) (Choi
% et al., 2021) which is initially proposed for super-resolution (SR) tasks considers the
% following update for line 5 (Alg. 1):
% xt−1 = x′
% t−1 + ζILVR · A†(yt−1 − Ax′
% t−1),
% where ζILVR is the weighting parameter, A† is the left-pseudoinverse of the forward
% operator, and yt−1 ∼ q(yt−1|y0).
% • Select 3 images from each dataset’s validation set provided to you (click) and
% place them in their corresponding folders under "./step1 utils/data/". Note:
% Their pre-trained networks are different so do not forget to change the --dataset
% parser for the correct network prior. Also do not forget to have some fun so choose
% the celebrities you know! Let’s see if you can recognize them after reconstruction!
% • Implement the q sample() function to perform the forward noising process de-
% fined as q(xt−1 | x0) = N (xt−1; √¯αt−1 x0, (1 − ¯αt−1)I).
% • Implement the ilvr() function to perform ILVR’s ∇xt p(y|x) update. Use the
% implemented q sample() function to obtain the noisy measurements yt−1.
% • Perform image restoration for the following inverse problem tasks: (i) SR×4, (ii)
% SR×8, (iii) 80% random inpainting, and (iv) 128×128 box inpainting. Assume
% σy = 0 and use 1000 sampling steps. Tune ζILVR heuristically for best per-
% formance, and report reference, measurement, and reconstruction for each image
% along with their corresponding PSNR, SSIM, and LPIPS metrics. Note:
% These degradations are already defined for you. You only need to change them
% from parser operations.
% • Discuss the observed performance and report the restoration time per image.
% Do you obtain similar results for inpainting tasks as for super-resolution tasks? If
% not, why might that be the case?
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/ILVR/SR_x4/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/ILVR/SR_x8/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/ILVR/Inpainting_random_80pct/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/ILVR/Inpainting_box/recon_3.png}
    \caption{ILVR Reconstructions on CelebA-HQ for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:ilvr_celeba}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/ILVR/SR_x4/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/ILVR/SR_x8/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/ILVR/Inpainting_random_80pct/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/ILVR/Inpainting_box/recon_1.png}
    \caption{ILVR Reconstructions on ImageNet for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:ilvr_imagenet}
\end{figure}





I found that an ILVR weight of 0.8 worked well during initial tests, so I used that value for all tasks. For this task I inlcuded the results from CelebA-HQ and ImageNet datasets in Figures~\ref{fig:ilvr_celeba} and \ref{fig:ilvr_imagenet}. 

The CelebA model had an average performance of:
\begin{list}{-}{ }
    \item Time: 50s
    \item Weight: 0.8
    \item SRx4:
    \subitem PSNR: 30.97
    \subitem SSIM: 0.880
    \subitem LPIPS: 0.0729
    \item SRx8:
    \subitem PSNR: 26.44
    \subitem SSIM: 0.7464
    \subitem LPIPS: 0.1243
    \item 80\% random inpainting:
    \subitem PSNR: 20.86
    \subitem SSIM: 0.5541
    \subitem LPIPS: 0.4891
    \item 128x128 box inpainting:
    \subitem PSNR: 20.33
    \subitem SSIM: 0.8244
    \subitem LPIPS: 0.1231
\end{list}

The ImageNet model had an average performance of:
\begin{list}{-}{ }
    \item Time: 176s
    \item Weight: 0.8
    \item SRx4:
        \subitem PSNR: 25.33
        \subitem SSIM: 0.735
        \subitem LPIPS: 0.2653
    \item SRx8:
        \subitem PSNR: 22.10
        \subitem SSIM: 0.5647
        \subitem LPIPS: 0.2642
    \item 80\% random inpainting:
        \subitem PSNR: 15.95
        \subitem SSIM: 0.1889
        \subitem LPIPS: 0.9146
    \item 128x128 box inpainting:
        \subitem PSNR: 17.23
        \subitem SSIM: 0.7826
        \subitem LPIPS: 0.2218
\end{list}

The ILVR method performed the best out of all the methods I implemented. If I had to guess, this is because the direct projection step makes the best use of the information from the measurement, and keeping the diffusion model step separate allows it to not be biased by the measurement. 

Across the four tasks, the celeba model looked the best visually, and had higher PSNR and SSIM values. The majority of the results I will be talking about qualitatively are from the CelebA model, as the ImageNet model were noiser on average.

For the super-resolution tasks, The model did quite admerably, with the SRx4 task looking very close to the original image, and the SRx8 suprisingly close given how visually different the low-res input was. For the random inpainting task, The noise was still present in the output, but it was noticably cleaner than the input and the noise was around the edges. For the box inpainting task, while the face that was inpainted did not look like Bryan Cranston, I could believe that it was a face. Given the amount of missing information, it is reasonable that the model would not be able to reconstruct Bryan Cranston exactly.

Because of the amount of missing information in the last two tasks, it could be argued that there exist many plausable images that could fit the compresed measurement, so the model should not reasonalby be expected to reconstruct the original image exactly. However, the model still did a good job of creating images that were consistent with the measurements and looked realistic.


\subsubsection{C: Manifold Constrained Gradient (MCG)}  
% (c) [6 pts] Manifold Constrained Gradient (MCG) (Chung et al., 2022) improves upon
% ILVR by first taking a gradient step along the manifold followed by a projection step
% similar to ILVR (but with A⊤ instead of A†):
% ˜xt−1 = x′
% t−1 − ζMCG · ∇xt ||y − Aˆx0|t||2
% xt−1 = x′
% t−1 + A⊤(yt−1 − A˜xt−1)
% • Implement the mcg() function and perform image restoration for the following
% inverse problem tasks: (i) SR×4, (ii) SR×8, (iii) 80% random inpainting, and
% (iv) 128×128 box inpainting. Assume σy = 0 and use 1000 sampling steps.
% Tune ζMCG heuristically for best performance, and report reference, measurement,
% and reconstruction for each image along with their corresponding PSNR, SSIM,
% and LPIPS metrics. Note: Use the same 3 images from part (b) and obtain
% yt−1 similar to ILVR using q sample() function.
% 3
% • Discuss the observed performance and report the restoration time per image.
% Compare the speed and quality to ILVR. Do you see consistent performance across
% restoration tasks this time? Although MCG also requires 1000 sampling (denois-
% ing) steps, do you notice any slowness compared to ILVR? Discuss.





\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/MCG/SR_x4/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/MCG/SR_x8/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/MCG/Inpainting_random_80pct/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/MCG/Inpainting_box/recon_3.png}
    \caption{MCG Reconstructions on CelebA-HQ for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:mcg_celeba}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/MCG/SR_x4/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/MCG/SR_x8/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/MCG/Inpainting_random_80pct/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/MCG/Inpainting_box/recon_1.png}
    \caption{MCG Reconstructions on ImageNet for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:mcg_imagenet}
\end{figure}


MCG produced some very sharp results for the super resolution tasks, as seen in Figures~\ref{fig:mcg_celeba} and \ref{fig:mcg_imagenet}, but this also resulted in very notable artivacts. These are much worse than in the ILVR results, and the PSNR and SSIM values reflect this, especially for the SRx4 task on the ImageNet model. This is likely because the gradient step pushes the image to be more consistent with the measurement, but might create non realistic images.

For the 80\% random inpainting task, the result was much cleaner, with no residual noise, and that is reflected in the PSNR and SSIM values that were higher for both models, although ImageNet had some artifacting. 

For the box inpainting task, the model failed on both datasetsl, and reconstructed the idea of a face, but it was had a rainbow texture and did not look realistic at all. Image net was not better on this task. It is likely that the gradient step pushed the image away from the data manifold, and the projection step was not able to recover a realistic image.

I think that it might have been a bit slower, but I did not notice a significant difference in speed compared to ILVR on my machine. 

\begin{list}{-}{ }
    \item Time: 55s
    \item Weight: 0.5
    \item SRx4:
        \subitem PSNR: 19.68
        \subitem SSIM: 0.8047
        \subitem LPIPS: 0.1846
    \item SRx8:
        \subitem PSNR: 25.68
        \subitem SSIM: 0.7404
        \subitem LPIPS: 0.1335
    \item 80\% random inpainting:
        \subitem PSNR: 33.40
        \subitem SSIM: 0.9260
        \subitem LPIPS: 0.0359
    \item 128x128 box inpainting:
        \subitem PSNR: 19.68
        \subitem SSIM:0.8047
        \subitem LPIPS: 0.1856
\end{list}



\begin{list}{-}{ }
    \item Time: 178s
    \item Weight: 0.5
    \item SRx4:
        \subitem PSNR: 12.75
        \subitem SSIM: 0.3567
        \subitem LPIPS: 0.6224
    \item SRx8:
        \subitem PSNR: 14.46
        \subitem SSIM: 0.3693
        \subitem LPIPS: 0.4716
    \item 80\% random inpainting:
        \subitem PSNR: 23.98
        \subitem SSIM: 0.7838
        \subitem LPIPS: 0.2242
    \item 128x128 box inpainting:
        \subitem PSNR: 14.11
        \subitem SSIM: 0.7472
        \subitem LPIPS: 0.3034
\end{list}




\subsubsection{D: Denoising Diffusion Null-Space Model (DDNM)}
% (d) [6 pts] Denoising Diffusion Null-Space Model (DDNM) (Wang et al., 2023) replaces
% the line 5 update in Alg. 1 with an update from a range-null-space decomposition of
% the forward operator. Specifically, at each step, it forms the Tweedie estimate ˆx0|t and
% projects it onto the affine constraint set as:
% ˜x0|t = ˆx0|t + ζDDNM · A†(y − Aˆx0|t).
% The goal here is to enforce exact data consistency in the range of A, while preserving
% the null-space component favored by the prior. Once this refined denoised estimate
% is obtained, DDNM gets the final sample as x′
% t−1 ∼ p(xt−1|xt, ˜x0|t). Given this new
% formulation, DDNM improves upon past methods in terms of required sampling steps.
% • Implement the ddnm() function and perform image restoration for the following
% inverse problem tasks: (i) SR×4, (ii) SR×8, (iii) 80% random inpainting, and (iv)
% 128×128 box inpainting. Assume σy = 0 and use 100 sampling steps. Tune
% ζDDNM heuristically for best performance, and report reference, measurement, and
% reconstruction for each image along with their corresponding PSNR, SSIM, and
% LPIPS metrics. Note: Use the same 3 images from part (b).
% • Discuss the observed performance and report the restoration time per image.
% Compare the speed and quality to ILVR and MCG.



\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DDNM/SR_x4/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DDNM/SR_x8/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DDNM/Inpainting_random_80pct/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DDNM/Inpainting_box/recon_3.png}
    \caption{DDNM Reconstructions on CelebA-HQ for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:ddnm_celeba}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DDNM/SR_x4/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DDNM/SR_x8/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DDNM/Inpainting_random_80pct/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DDNM/Inpainting_box/recon_1.png}
    \caption{DDNM Reconstructions on ImageNet for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting.}
    \label{fig:ddnm_imagenet}
\end{figure}

This model was much faster because it only used 100 sampling steps, but the results were proportionally worse than the previous two methods. The results are shown in Figures~\ref{fig:ddnm_celeba} and \ref{fig:ddnm_imagenet}. Comparing the PSNR and SSIM values to ILVR and MCG, they are significantly lower quality and I would not use this method if quality is a priority.

The super-resolution tasks were very noisy, and had a wavey texture to them. While the box inpainting task was able to make the face and origonal image brighter, there was still a lot of noise and very few details. For some reason the random inpainting task was the worst, with almost pure noise in the box. 

One explination for this would be that with only 100 steps, the model is not able to denoise the image enough, resulting in a noisy output. It is also possible I did not find a good weight for the projection step, but I tried a range of values and none produced good results.

\begin{list}{-}{ }
    \item Time: 3s
    \item Weight: 1.0
    \item SRx4:
        \subitem PSNR: 18.25
        \subitem SSIM: 0.2746
        \subitem LPIPS: 0.8742
    \item SRx8:
        \subitem PSNR: 17.02
        \subitem SSIM: 0.2033
        \subitem LPIPS: 1.0188
    \item 80\% random inpainting:
        \subitem PSNR: 10.87
        \subitem SSIM: 0.0629
        \subitem LPIPS: 14.827
    \item 128x128 box inpainting:
        \subitem PSNR: 15.83
        \subitem SSIM: 0.7219
        \subitem LPIPS: 0.5116
\end{list}

\begin{list}{-}{ }
    \item Time: 9s
    \item Weight: 1.0
    \item SRx4:
        \subitem PSNR: 13.33
        \subitem SSIM: 0.3296
        \subitem LPIPS: 0.8221
    \item SRx8:
        \subitem PSNR: 12.76
        \subitem SSIM: 0.2855
        \subitem LPIPS: 0.9485
    \item 80\% random inpainting:
        \subitem PSNR: 8.03
        \subitem SSIM: 0.0324
        \subitem LPIPS: 1.4114
    \item 128x128 box inpainting:
        \subitem PSNR: 13.24
        \subitem SSIM: 0.7202
        \subitem LPIPS: 0.5390
\end{list}

\subsubsection{E: Noisy measurements and Diffusion Posterior Sampling (DPS)}
% (e) [5 pts] Add noise to the measurements with σy = 0.05 (use --sigma y). Repeat steps
% (b), (c), and (d) only for the SR×4 and box inpainting tasks on a single dataset (either
% CelebA-HQ or ImageNet). Report reference, measurement, and reconstruction for each
% image. Do you observe any performance degradation for ILVR, MCG, or DDNM? Is
% the drop in performance more pronounced for one inverse task compared to the other
% (SR×4 vs box inpainting)? Explain your reasoning for each case.

Refer to the appendix for the figures for the noisy ILVR, MCG, and DDNM results (\ref{fig:noisy_ddnm_celeba_sr_x4}, \ref{fig:noisy_ddnm_celeba_sr_x8}, \ref{fig:noisy_ddnm_celeba_inpainting_random_80pct}, \ref{fig:noisy_ddnm_celeba_inpainting_box}). The genral trend is that the noisy measurements resulted in worse reconstructions across all three methods. We only use the CelebA-HQ dataset for this task. The settings for the three methods were the same as before, with only the addition of noise to the measurements.

For the spesific numbers for the noise statistics, please refer to the figures. We find that the PSNR seems universlly higher, with one exception on the ILVR at 80\% random inpainting task. This seems to indicate that the noise makes it harder to reconstruct the image accurately. However, the images are still reasonable for the super-resolution tasks, but the box inpainting task has less realistic noise patterns inside the box. 



\subsubsection{F: Diffusion Posterior Sampling (DPS) for noisy measurements}
% (f) [6 pts] Diffusion Posterior Sampling (DPS) (Chung et al., 2023) applies MCG up-
% date for the most part but it removes the final projection step for stability to noisy
% conditions. Therefore, its update is given as:
% xt−1 = x′
% t−1 − ζDPS · ∇xt ||y − Aˆx0|t||2
% • Implement the dps() function and perform image restoration for the following
% inverse problem tasks: (i) SR×4, (ii) SR×8, (iii) 80% random inpainting, and
% (iv) 128×128 box inpainting. Assume σy = 0.05 and use 1000 sampling steps.
% Tune ζDPS heuristically for best performance, and report reference, measurement,
% and reconstruction for each image along with their corresponding PSNR, SSIM,
% and LPIPS metrics. Note: Use the same 3 images from part (b).
% • Discuss the observed performance and report the restoration time per image.
% Compare the speed and quality to ILVR, MCG and DDNM. Do you see improved
% performance across noisy conditions? Discuss


\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DPS/SR_x4/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DPS/SR_x8/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DPS/Inpainting_random_80pct/recon_3.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/CelebA_HQ/DPS/Inpainting_box/recon_3.png}
    \caption{DPS Reconstructions on CelebA-HQ for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting with noisy measurements.}
    \label{fig:dps_celeba}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DPS/SR_x4/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DPS/SR_x8/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DPS/Inpainting_random_80pct/recon_1.png}
    \includegraphics[width=0.9\columnwidth]{images/step1_results/ImageNet/DPS/Inpainting_box/recon_1.png}
    \caption{DPS Reconstructions on ImageNet for SRx4, SRx8, 80\% random inpainting, and 128x128 box inpainting with noisy measurements.}
    \label{fig:dps_imagenet}
\end{figure}

This method produced a lot of wav ey artifacts in the super-resolution tasks for both CelebA-HQ and ImageNet. Otherwise, the face is very crisp and believeable in CelebA, whreeas the ImageNet model struggled on super-resolution.

For the random inpainting task, the model removed a lot of the noise, but there was still some in the background of both models. For the ImageNet model, this introduced some artifacts. 

The box inpainting task very bad compared to the first method, where there was sturctured rainbow noise in the box, and it also created noisy patterns in the rest of the image. Image net was a little better on this task, but still had a lot of noise. This is reflected in the PSNR and SSIM values, which were much lower than the original MCG method.

One reason why this may be is that without the projection step, the gradient step may push the image off the data manifold, resulting in noisy images. This is more pronounced given I couldn't find a good weight that produced good results.


\begin{list}{-}{ }
    \item Time: 55s
    \item Weight: 1.0
    \item SRx4:
        \subitem PSNR: 22.94
        \subitem SSIM: 0.7031
        \subitem LPIPS: 0.2730
    \item SRx8:
        \subitem PSNR: 26.43
        \subitem SSIM: 0.7592
        \subitem LPIPS: 0.1143
    \item 80\% random inpainting:
        \subitem PSNR: 19.02
        \subitem SSIM: 0.7389
        \subitem LPIPS: 0.3173
    \item 128x128 box inpainting:
        \subitem PSNR: 11.92
        \subitem SSIM: 0.3863
        \subitem LPIPS: 0.5397
\end{list}


\begin{list}{-}{ }
    \item Time: 177s
    \item Weight: 1.0
    \item SRx4:
        \subitem PSNR: 13.22
        \subitem SSIM: 0.3813
        \subitem LPIPS: 0.4515
    \item SRx8:
        \subitem PSNR: 16.65
        \subitem SSIM: 0.4041
        \subitem LPIPS: 0.3862
    \item 80\% random inpainting:
        \subitem PSNR: 19.34
        \subitem SSIM: 0.5619
        \subitem LPIPS: 0.4727
    \item 128x128 box inpainting:
        \subitem PSNR: 13.69
        \subitem SSIM: 0.5530
        \subitem LPIPS: 0.4243
\end{list}